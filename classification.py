# -*- coding: utf-8 -*-
"""Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16I-LxE63PWj7D_j2ML7MnnEZruOg85Id
"""

!nvidia-smi

from google.colab import drive
drive.mount('/gdrive')

c_dir='/gdrive/My Drive/keras-covid-19/dataset/covid'
n_dir='/gdrive/My Drive/keras-covid-19/dataset/normal'

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import cv2
import numpy as np
import tensorflow as tf
from tensorflow.python import keras
from keras import backend as K
from keras import layers
import seaborn as sns
from keras.models import Sequential,Model
from keras.layers import  Conv2D,MaxPooling2D,Dropout,BatchNormalization,Flatten,Dense,GlobalAveragePooling2D,AveragePooling2D
from keras.optimizers import Adam,SGD,RMSprop
from keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

import matplotlib.pyplot as plt

import os 
  
# Function to rename multiple files 
 
for count, filename in enumerate(os.listdir(c_dir)): 
        dst ="covid_" + str(count) + ".jpg"
        src = c_dir+'/'+ filename 
        dst = c_dir+'/'+ dst 
          
        # rename() function will 
        # rename all the files 
        os.rename(src, dst)

for count, filename in enumerate(os.listdir(n_dir)): 
        dst ="normal_" + str(count) + ".jpg"
        src = n_dir+'/'+ filename 
        dst = n_dir+'/'+ dst 
          
        # rename() function will 
        # rename all the files 
        os.rename(src, dst)

image_path='/gdrive/My Drive/keras-covid-19/dataset'
folder=os.listdir()

data = []
labels = []
import glob
# loop over the image paths
for i in glob.glob('/gdrive/My Drive/keras-covid-19/dataset/*/*.jpg'):
       
 label = i.split('_')[0]
 image = cv2.imread(i)
 image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
 image = cv2.resize(image, (224, 224))
 data.append(image)
 labels.append(label)
data=np.array(data,dtype=float) / 255.0
labels = np.array(labels)
labels1 = np.array(labels)
# convert the data and labels to NumPy arrays while scaling the pixel
# intensities to the range [0, 255]

lb = LabelBinarizer()
labels = lb.fit_transform(labels)
labels = to_categorical(labels)

(trainX, testX, trainY, testY) = train_test_split(data, labels,
	test_size=0.20, stratify=labels, random_state=42)

trainAug = ImageDataGenerator(
	width_shift_range=0.1,
  height_shift_range=0.1,
	 shear_range = 0.2,  
	 rotation_range=15, 
 horizontal_flip = True,    
 	
	fill_mode="nearest")

if K.image_data_format()=='channels_first':  # (‘channels_last’,‘channels_first’）
   input_shape =(3,224,224)
else:
   input_shape =(224,224,3)   
   
# The input feature is a tensor of 150x150x3, where 150x150 is used for image pixels and 3 is used for three color channels
img_input = layers.Input(shape=input_shape)

#
# The first convolution layer extracts the features of 3x3x32 (size of 2D convolution window: 3x3, output 32 dimensions),
#uses a linear rectification function (Rectified Linear Unit, ReLU), and then the largest pooling layer with a size of 2x2

x = layers.Conv2D(32,3,activation='relu')(img_input)
x = layers.MaxPooling2D(2)(x)

x = layers.Conv2D(64,3,activation='relu')(x)
x = layers.MaxPooling2D(2)(x)

x = layers.Conv2D(64,3,activation='relu')(x)
x = layers.MaxPooling2D(2)(x)


# Flatten the feature map into a one-dimensional data (`1-dim`) tensor to add a fully connected layer (dense)

x = layers.Flatten()(x)


# Use `sigmoid` activation function and 128 neurons to create a fully connected layer

x = layers.Dense(128,activation='sigmoid')(x)

# Randomly disconnect the input neural cloud with a certain probability to prevent overfitting
x = layers.Dropout(0.5)(x)                       


# Create output layer with 2 neurons and `softmax` activation function

output = layers.Dense(2,activation='softmax')(x)

model= Model(img_input,output)

EPOCHS = 20
BS = 10

def focal_loss(gamma=2., alpha=4.):

    gamma = float(gamma)
    alpha = float(alpha)

    def focal_loss_fixed(y_true, y_pred):
        """Focal loss for multi-classification
        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)
        Notice: y_pred is probability after softmax
        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper
        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)
        Focal Loss for Dense Object Detection
        https://arxiv.org/abs/1708.02002

        Arguments:
            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]
            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]

        Keyword Arguments:
            gamma {float} -- (default: {2.0})
            alpha {float} -- (default: {4.0})

        Returns:
            [tensor] -- loss.
        """
        epsilon = 1.e-9
        y_true = tf.convert_to_tensor(y_true, tf.float32)
        y_pred = tf.convert_to_tensor(y_pred, tf.float32)

        model_out = tf.add(y_pred, epsilon)
        ce = tf.multiply(y_true, -tf.math.log(model_out))
        weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))
        fl = tf.multiply(alpha, tf.multiply(weight, ce))
        reduced_fl = tf.reduce_max(fl, axis=1)
        return tf.reduce_mean(reduced_fl)
    return focal_loss_fixed

opt = Adam(lr=0.00001)
model.compile(loss=focal_loss(alpha=1), optimizer=opt,
	metrics=["accuracy"])

# train the head of the network
print("[INFO] training head...")
H = model.fit_generator(
	trainAug.flow(trainX, trainY, batch_size=BS),
	steps_per_epoch=len(trainX) // BS,
	validation_data=(testX, testY),
	validation_steps=len(testX) // BS,
	epochs=EPOCHS)

# make predictions on the testing set
print("[INFO] evaluating network...")
predIdxs = model.predict(testX, batch_size=BS)

# for each image in the testing set we need to find the index of the
# label with corresponding largest predicted probability
predIdxs = np.argmax(predIdxs, axis=1)
classes=['Covid','Normal']
# show a nicely formatted classification report
print(classification_report(testY.argmax(axis=1), predIdxs,
	target_names=classes))

# compute the confusion matrix and and use it to derive the raw
# accuracy, sensitivity, and specificity
cm = confusion_matrix(testY.argmax(axis=1), predIdxs)
total = sum(sum(cm))
acc = (cm[0, 0] + cm[1, 1]) / total
sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])
specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])

# show the confusion matrix, accuracy, sensitivity, and specificity
print(cm)
print("acc: {:.4f}".format(acc))
print("sensitivity: {:.4f}".format(sensitivity))
print("specificity: {:.4f}".format(specificity))

# plot the training loss and accuracy
N = EPOCHS
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, N), H.history["accuracy"], label="train_acc")
plt.plot(np.arange(0, N), H.history["val_accuracy"], label="val_acc")
plt.title("Training Loss and Accuracy on COVID-19 Dataset")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower left")

sns.heatmap(cm, annot=True, cbar=False)
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.title('Confusion Matrix')

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import auc


pred = model.predict(testX, batch_size=BS)
pred = np.argmax(pred,axis=1).ravel()

fpr_keras, tpr_keras, thresholds_keras = roc_curve(testY.argmax(axis=1),pred)
auc_keras = auc(fpr_keras, tpr_keras)
plt.figure(1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_keras, tpr_keras, label='CNN model')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve')
plt.legend(loc='best')
plt.show()